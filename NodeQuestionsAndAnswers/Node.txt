Q1) What is node js?
Ans:-
-----
        Node.js is an open-source and cross-platform JavaScript runtime environment. It is a popular tool 
for almost any kind of project!Node.js runs the V8 JavaScript engine, the core of Google Chrome,
outside of the browser. This allows Node.js to be very performant.A Node.js app runs in a single process, 
without creating a new thread for every request. Node.js provides a set of asynchronous I/O primitives in 
mits standard library that prevent JavaScript code from blocking and generally, libraries in Node.js are
written using non-blocking paradigms, making blocking behavior the exception rather than the norm.
When Node.js performs an I/O operation, like reading from the network, accessing a database or the filesystem, 
instead of blocking the thread and wasting CPU cycles waiting, Node.js will resume the operations when the response
comes back.This allows Node.js to handle thousands of concurrent connections with a single server without introducing
the burden of managing thread concurrency, which could be a significant source of bugs.
Node.js has a unique advantage because millions of frontend developers that write JavaScript for the browser are 
now able to write the server-side code in addition to the client-side code without the need to learn a completely 
different language.In Node.js the new ECMAScript standards can be used without problems, as you don't have to wait
for all your users to update their browsers - you are in charge of deciding which ECMAScript version to use by
changing the Node.js version, and you can also enable specific experimental features by running Node.js with flags.


Q2) Differences between Node.js and the Browser?
Ans:-
------
        Both the browser and Node.js use JavaScript as their programming language. Building apps that run in the browser
is a completely different thing than building a Node.js application. Despite the fact that it's always JavaScript, 
there are some key differences that make the experience radically different.
From the perspective of a frontend developer who extensively uses JavaScript, Node.js apps bring with them a huge
advantage: the comfort of programming everything - the frontend and the backend - in a single language.
You have a huge opportunity because we know how hard it is to fully, deeply learn a programming language, and by using
the same language to perform all your work on the web - both on the client and on the server, you're in a unique
position of advantage.

**What changes is the ecosystem**
----------------------------------

In the browser, most of the time what you are doing is interacting with the DOM, or other Web Platform APIs like Cookies.
Those do not exist in Node.js, of course. You don't have the document, window and all the other objects that are provided by the browser.
And in the browser, we don't have all the nice APIs that Node.js provides through its modules, like the filesystem access functionality.
Another big difference is that in Node.js you control the environment. Unless you are building an open source application that anyone can 
deploy anywhere, you know which version of Node.js you will run the application on. Compared to the browser environment, where you don't get 
the luxury to choose what browser your visitors will use, this is very convenient.
This means that you can write all the modern ES2015+ JavaScript that your Node.js version supports. Since JavaScript
moves so fast, but browsers can be a bit slow to upgrade, sometimes on the web you are stuck with using older
JavaScript / ECMAScript releases. You can use Babel to transform your code to be ES5-compatible before shipping it to the browser, but in Node.js, you won't need that.
Another difference is that Node.js supports both the CommonJS and ES module systems (since Node.js v12), while in the browser we are starting
to see the ES Modules standard being implemented.
In practice, this means that you can use both require() and import in Node.js, while you are limited to import in 
the browser.

Q3) What is V8?
Ans:-
--------
V8 is the name of the JavaScript engine that powers Google Chrome. It's the thing that takes our JavaScript
and executes it while browsing with Chrome. V8 is the JavaScript engine i.e. it parses and executes JavaScript code.
The DOM, and the other Web Platform APIs (they all makeup runtime environment) are provided by the browser.
The cool thing is that the JavaScript engine is independent of the browser in which it's hosted. This key feature
enabled the rise of Node.js. V8 was chosen to be the engine that powered Node.js back in 2009, and as the popularity 
of Node.js exploded, V8 became the engine that now powers an incredible amount of server-side code written in JavaScript.
The Node.js ecosystem is huge and thanks to V8 which also powers desktop apps, with projects like Electron.

Other JS engines
----------------
Other browsers have their own JavaScript engine:
--Firefox has SpiderMonkey
--Safari has JavaScriptCore (also called Nitro)
--Edge was originally based on Chakra but has more recently been rebuilt using Chromium and the V8 engine.
and many others exist as well.
All those engines implement the ECMA ES-262 standard, also called ECMAScript, the standard used by JavaScript.
The quest for performance
V8 is written in C++, and it's continuously improved. It is portable and runs on Mac, Windows, Linux and several
other systems.
In this V8 introduction, we will ignore the implementation details of V8: they can be found on more authoritative
sites (e.g. the V8 official site), and they change over time, often radically.
V8 is always evolving, just like the other JavaScript engines around, to speed up the Web and the Node.js ecosystem.
On the web, there is a race for performance that's been going on for years, and we (as users and developers) benefit
a lot from this competition because we get faster and more optimized machines year after year.

Compilation
-----------
JavaScript is generally considered an interpreted language, but modern JavaScript engines no longer just interpret
JavaScript, they compile it.This has been happening since 2009, when the SpiderMonkey JavaScript compiler was added
to Firefox 3.5, and everyone followed this idea.
JavaScript is internally compiled by V8 with just-in-time (JIT) compilation to speed up the execution.
This might seem counter-intuitive, but since the introduction of Google Maps in 2004, JavaScript has evolved from a
language that was generally executing a few dozens of lines of code to complete applications with thousands to hundreds 
of thousands of lines running in the browser.
Our applications can now run for hours inside a browser, rather than being just a few form validation rules or simple scripts.
In this new world, compiling JavaScript makes perfect sense because while it might take a little bit more to have the
JavaScript ready, once done it's going to be much more performant than purely interpreted code.

Q4) What is npm?
Ans:-
-----
    npm is the standard package manager for Node.js.

    You can also install a specific package by running
    ---------------------------------------------------
    npm install <package-name>

--save-dev installs and adds the entry to the package.json file devDependencies
--no-save installs but does not add the entry to the package.json file dependencies
--save-optional installs and adds the entry to the package.json file optionalDependencies
--no-optional will prevent optional dependencies from being installed

Shorthands of the flags can also be used:

-S: --save
-D: --save-dev
-O: --save-optional

The difference between devDependencies and dependencies is that the former contains development tools, 
like a testing library, while the latter is bundled with the app in production.
As for the optionalDependencies the difference is that build failure of the dependency will not cause
installation to fail. But it is your program's responsibility to handle the lack of the dependency.
Read more about optional dependencies.

Versioning
-----------
In addition to plain downloads, npm also manages versioning, so you can specify any specific version of a
package, or require a version higher or lower than what you need.
Many times you'll find that a library is only compatible with a major release of another library.

You can install a specific version of a package, by running
-----------------------------------------------------------
npm install <package-name>@<version>

Q5) Difference b/w blocking and non blocking?
Ans:-
-----
This overview covers the difference between blocking and non-blocking calls in Node.js. This overview will
refer to the event loop and libuv but no prior knowledge of those topics is required. Readers are assumed to
have a basic understanding of the JavaScript language and Node.js callback pattern.

"I/O" refers primarily to interaction with the system's disk and network supported by libuv.

Blocking
--------
Blocking is when the execution of additional JavaScript in the Node.js process must wait until a non-JavaScript
operation completes. This happens because the event loop is unable to continue running JavaScript while a blocking 
operation is occurring.
In Node.js, JavaScript that exhibits poor performance due to being CPU intensive rather than waiting on a 
non-JavaScript operation, such as I/O, isn't typically referred to as blocking. Synchronous methods in the Node.js 
standard library that use libuv are the most commonly used blocking operations. Native modules may also have blocking 
methods.
All of the I/O methods in the Node.js standard library provide asynchronous versions, which are non-blocking, and 
accept callback functions. Some methods also have blocking counterparts, which have names that end with Sync.

Comparing Code
---------------------------------------------------------------------------------------
Blocking methods execute synchronously and non-blocking methods execute asynchronously.
Using the File System module as an example, this is a synchronous file read:

const fs = require('node:fs');
const data = fs.readFileSync('/file.md'); // blocks here until file is read
----------------------------------------------------------------------------
And here is an equivalent asynchronous example:

const fs = require('node:fs');
fs.readFile('/file.md', (err, data) => {
  if (err) throw err;
});

The first example appears simpler than the second but has the disadvantage of the second line blocking the 
execution of any additional JavaScript until the entire file is read. Note that in the synchronous version 
if an error is thrown it will need to be caught or the process will crash. In the asynchronous version, it
is up to the author to decide whether an error should throw as shown.

Let's expand our example a little bit:
--------------------------------------
const fs = require('node:fs');
const data = fs.readFileSync('/file.md'); // blocks here until file is read
console.log(data);
moreWork(); // will run after console.log
----------------------------------------------
And here is a similar, but not equivalent asynchronous example:

const fs = require('node:fs');
fs.readFile('/file.md', (err, data) => {
  if (err) throw err;
  console.log(data);
});
moreWork(); // will run before console.log
----------------------------------------------

In the first example above, console.log will be called before moreWork(). In the second example 
fs.readFile() is non-blocking so JavaScript execution can continue and moreWork() will be called first.
The ability to run moreWork() without waiting for the file read to complete is a key design choice that
allows for higher throughput.

Concurrency and Throughput
JavaScript execution in Node.js is single threaded, so concurrency refers to the event loop's capacity to
execute JavaScript callback functions after completing other work. Any code that is expected to run in a 
concurrent manner must allow the event loop to continue running as non-JavaScript operations, like I/O, are occurring.
As an example, let's consider a case where each request to a web server takes 50ms to complete and 45ms of that 50ms 
is database I/O that can be done asynchronously. Choosing non-blocking asynchronous operations frees up that 45ms 
per request to handle other requests. This is a significant difference in capacity just by choosing to use non-blocking
methods instead of blocking methods.
The event loop is different than models in many other languages where additional threads may be created to handle 
concurrent work.

Q6) What is JavaScript Asynchronous Programming and Callbacks?
Ans:-
------
Asynchronous means that things can happen independently of the main program flow.
In the current consumer computers, every program runs for a specific time slot and then it stops its 
execution to let another program continue their execution. This thing runs in a cycle so fast that it's 
impossible to notice. We think our computers run many programs simultaneously, but this is an illusion 
(except on multiprocessor machines).
Programs internally use interrupts, a signal that's emitted to the processor to gain the attention of the system.
Let's not go into the internals of this now, but just keep in mind that it's normal for programs to be asynchronous 
and halt their execution until they need attention, allowing the computer to execute other things in the meantime.
When a program is waiting for a response from the network, it cannot halt the processor until the request finishes.
Normally, programming languages are synchronous and some provide a way to manage asynchronicity in the language
or through libraries. C, Java, C#, PHP, Go, Ruby, Swift, and Python are all synchronous by default. Some of them 
handle async operations by using threads, spawning a new process.

**JavaScript
------------
JavaScript is synchronous by default and is single threaded. This means that code cannot create new threads and
run in parallel.

Lines of code are executed in series, one after another, for example:
---------------------------------------------------------------------
const a = 1;
const b = 2;
const c = a * b;
console.log(c);
doSomething();
-------------------------
But JavaScript was born inside the browser, its main job, in the beginning, was to respond to user actions, 
like onClick, onMouseOver, onChange, onSubmit and so on. How could it do this with a synchronous programming model?
The answer was in its environment. The browser provides a way to do it by providing a set of APIs that can handle 
this kind of functionality.
More recently, Node.js introduced a non-blocking I/O environment to extend this concept to file access, network 
calls and so on.

Callbacks
----------
You can't know when a user is going to click a button. So, you define an event handler for the click event. This event
handler accepts a function, which will be called when the event is triggered:
document.getElementById('button').addEventListener('click', () => {
  // item clicked
});

This is the so-called callback.

A callback is a simple function that's passed as a value to another function, and will only be executed when the 
event happens. We can do this because JavaScript has first-class functions, which can be assigned to variables and
passed around to other functions (called higher-order functions)
It's common to wrap all your client code in a load event listener on the window object, which runs the callback
function only when the page is ready:

window.addEventListener('load', () => {
  // window loaded
  // do what you want
});

Callbacks are used everywhere, not just in DOM events.
One common example is by using timers:

setTimeout(() => {
  // runs after 2 seconds
}, 2000);

**Handling errors in callbacks
------------------------------
How do you handle errors with callbacks? One very common strategy is to use what Node.js adopted: the
first parameter in any callback function is the error object: error-first callbacks
If there is no error, the object is null. If there is an error, it contains some description of the error
and other information.

const fs = require('node:fs');
fs.readFile('/file.json', (err, data) => {
  if (err) {
    // handle error
    console.log(err);
    return;
  }
  // no errors, process data
  console.log(data);
});

**The problem with callbacks
---------------------------
Callbacks are great for simple cases!
However every callback adds a level of nesting, and when you have lots of callbacks, the code starts to be
complicated very quickly:

window.addEventListener('load', () => {
  document.getElementById('button').addEventListener('click', () => {
    setTimeout(() => {
      items.forEach(item => {
        // your code here
      });
    }, 2000);
  });
});
JavaScript
This is just a simple 4-levels code, but I've seen much more levels of nesting and it's not fun.

How do we solve this?

Alternatives to callbacks
Starting with ES6, JavaScript introduced several features that help us with asynchronous code that do not 
involve using callbacks: Promises (ES6) and Async/Await (ES2017).

Q7) Event Emitter in Node js?
Ans:-
-----
  If you worked with JavaScript in the browser, you know how much of the interaction of the user is handled
through events: mouse clicks, keyboard button presses, reacting to mouse movements, and so on.
On the backend side, Node.js offers us the option to build a similar system using the events module.
This module, in particular, offers the EventEmitter class, which we'll use to handle our events.

You initialize that using
--------------------------------------------
const EventEmitter = require('node:events');
const eventEmitter = new EventEmitter();
--------------------------------------------

This object exposes, among many others, the on and emit methods.

emit is used to trigger an event
on is used to add a callback function that's going to be executed when the event is triggered
For example, let's create a start event, and as a matter of providing a sample, we react to that by just logging to the console:

eventEmitter.on('start', () => {
  console.log('started');
});

When we run
eventEmitter.emit('start');

the event handler function is triggered, and we get the console log.
You can pass arguments to the event handler by passing them as additional arguments to emit():

eventEmitter.on('start', number => {
  console.log(`started ${number}`);
});
eventEmitter.emit('start', 23);

Multiple arguments
---------------------------------------------
eventEmitter.on('start', (start, end) => {
  console.log(`started from ${start} to ${end}`);
});
eventEmitter.emit('start', 1, 100);
---------------------------------------------

The EventEmitter object also exposes several other methods to interact with events, like

once(): add a one-time listener
removeListener() / off(): remove an event listener from an event
removeAllListeners(): remove all listeners for an event
You can read more about these methods in the official documentation.

Q8) How to read environment variables from Node.js?
Ans:-
------
The process core module of Node.js provides the env property which hosts all the environment variables that were 
set at the moment the process was started.

The below code runs app.js and set USER_ID and USER_KEY.
------------------------------------------
USER_ID=239482 USER_KEY=foobar node app.js
--------------------------------------------

That will pass the user USER_ID as 239482 and the USER_KEY as foobar. This is suitable for testing,
however for production, you will probably be configuring some bash scripts to export variables.
------------------------------------------------------------------------
Note: process does not require a "require", it's automatically available.
------------------------------------------------------------------------

Here is an example that accesses the USER_ID and USER_KEY environment variables, which we set in above code.
-------------------------------------
process.env.USER_ID; // "239482"
process.env.USER_KEY; // "foobar"
-------------------------------------

In the same way you can access any custom environment variable you set.
Node.js 20 introduced experimental support for .env files.
Now, you can use the --env-file flag to specify an environment file when running your Node.js application.
Here's an example .env file and how to access its variables using process.env.
---------------
# .env file
PORT=3000
---------------

In your js file
---------------------------
process.env.PORT; // "3000"
----------------------------

Run app.js file with environment variables set in .env file.
----------------------------
node --env-file=.env app.js
----------------------------

This command loads all the environment variables from the .env file, making them available to the application on
process.env
Also, you can pass multiple --env-file arguments. Subsequent files override pre-existing variables defined 
in previous files.
-------------------------------------------------------
node --env-file=.env --env-file=.development.env app.js
-------------------------------------------------------

Note: if the same variable is defined in the environment and in the file, the value from the environment
takes precedence.

Q9) What is REPL in node.js ?
Ans:-
-----
The node command is the one we use to run our Node.js scripts:
--------------
node script.js
--------------

If we run the node command without any script to execute or without any arguments, we start a REPL session:
------
node
------

Note:REPL stands for Read Evaluate Print Loop, and it is a programming language environment (basically a console window) 
that takes single expression as user input and returns the result back to the console after execution. 
The REPL session provides a convenient way to quickly test simple JavaScript code.

If you try it now in your terminal, this is what happens:
----------
â¯ node
>
----------

The command stays in idle mode and waits for us to enter something.
Tip: if you are unsure how to open your terminal, google "How to open terminal on your-operating-system".

The REPL is waiting for us to enter some JavaScript code, to be more precise.
Start simple and enter
----------------------
> console.log('test')
test
undefined
>
----------------------

The first value, test, is the output we told the console to print, then we get undefined which is the
return value of running console.log(). Node read this line of code, evaluated it, printed the result,
and then went back to waiting for more lines of code. Node will loop through these three steps for every
piece of code we execute in the REPL until we exit the session. That is where the REPL got its name.

Node automatically prints the result of any line of JavaScript code without the need to instruct it to do so.
For example, type in the following line and press enter:
---------------
> 5 === '5'
false
>
---------------

Note the difference in the outputs of the above two lines. The Node REPL printed undefined after executed
console.log(), while on the other hand, it just printed the result of 5 === '5'. You need to keep in mind
that the former is just a statement in JavaScript, and the latter is an expression.

In some cases, the code you want to test might need multiple lines. For example, say you want to define a 
function that generates a random number, in the REPL session type in the following line and press enter:
----------------------------
function generateRandom() {
...
----------------------------

The Node REPL is smart enough to determine that you are not done writing your code yet, and it will go into
a multi-line mode for you to type in more code. Now finish your function definition and press enter:
----------------------------
function generateRandom() {
...return Math.random()
}
undefined
----------------------------

The _ special variable
If after some code you type _, that is going to print the result of the last operation.

The Up arrow key
If you press the up arrow key, you will get access to the history of the previous lines of code executed 
in the current, and even previous REPL sessions.

Dot commands
The REPL has some special commands, all starting with a dot .. They are

.help: shows the dot commands help
.editor: enables editor mode, to write multiline JavaScript code with ease. Once you are in this mode, enter ctrl-D to run the code you wrote.
.break: when inputting a multi-line expression, entering the .break command will abort further input. Same as pressing ctrl-C.
.clear: resets the REPL context to an empty object and clears any multi-line expression currently being input.
.load: loads a JavaScript file, relative to the current working directory
.save: saves all you entered in the REPL session to a file (specify the filename)
.exit: exits the repl (same as pressing ctrl-C two times)
The REPL knows when you are typing a multi-line statement without the need to invoke .editor.

For example if you start typing an iteration like this:
-----------------------------
[1, 2, 3].forEach(num => {
-----------------------------  

and you press enter, the REPL will go to a new line that starts with 3 dots, indicating you can now continue to
work on that block.

-----------------------
... console.log(num)
... })
-----------------------
Bash
If you type .break at the end of a line, the multiline mode will stop and the statement will not be executed.

Run REPL from JavaScript file
We can import the REPL in a JavaScript file using repl.
----------------------------------
const repl = require('node:repl');
----------------------------------

Using the repl variable we can perform various operations. To start the REPL command prompt, type in the following line
-------------
repl.start();
-------------

Run the file in the command line.

---------------
node repl.js
---------------

You can pass a string which shows when the REPL starts. The default is '> ' (with a trailing space), but we 
can define custom prompt.

--------------------------------
// a Unix style prompt
const local = repl.start('$ ');
--------------------------------

You can display a message while exiting the REPL
--------------------------------
local.on('exit', () => {
  console.log('exiting repl');
  process.exit();
});
--------------------------------


Q10) What is Routing?
Ans:-
-------
    Routing defines the way in which the client requests are handled by the application endpoints.

**Types of url
-----------------
  1) File based url
  eg:- www.eirj.com/index.html
  in this example www.eirj.com is domain name and index.html is file 

  2) Resoure based url
  eg:- www.eirj.com/about 
  int this example www.eirj.com is domain name and about is Resoure 

  Routing basically means implementing diffrent actions for diffrent URLs.
  These actions can be implemented in diffrent ways, for example, by creating a function. 

**Route parameter
---------------- 
 eg:- www.eirj.com/about/id 
 in this example www.eirj.com is domain name, about is Resoure and id is parameter. 

**Query String 
----------------
 eg:- www.eirj.com/books?author=john&id=2 
 in this example after question mark whatever I have written is query string. 
 there is two query string author and id and both query strings are available in key value pair.


 Q11) Diffrence between search param and query params?
 Ans:- Both refer to the same thing â€” key-value pairs in the URL after ?
  The term â€œsearch paramsâ€ is used more often in frontend/browser code (with URLSearchParams).
  â€œquery paramsâ€ is the broader and more common term, especially in backend or API contexts.

  1. Query Parameters (Backend/API Perspective)
  
  -> Part of the URL after the ?

  -> Sent to the server in the HTTP request

  -> Used for filtering, pagination, searching, etc.
-------------------------------------------------
example-
  GET /users?page=2&limit=10
in node js with express js
  req.query.page // '2'
  req.query.limit // '10'
  -------------------------------------------------

2. Search Parameters (Frontend/Browser Perspective)

-> Term used in browser-side JavaScript (DOM APIs)

-> Accessed using the URLSearchParams API

------------------------------------------------
const params = new URLSearchParams(window.location.search);
const page = params.get('page'); // '2'
------------------------------------------------

URL Breakdown:
https://example.com/products?category=books&sort=price

 -> ?category=books&sort=price â†’ query string

 -> category and sort â†’ query/search parameters

 -> Use .query in backend (e.g., Express)

 -> Use URLSearchParams in frontend


 Q12) how to use commmand line in node js?

 Ans:- Node.js allows interaction with the command line through several methods. Here's a breakdown:
1. Running Node.js Scripts:
  -> Using the node command: The most common way is to use the node command followed by the script's file name.

  eg:-
  ----------------------
    node your_script.js

  -> Shebang: Adding a shebang line at the beginning of your script allows direct execution (on Unix-like systems).

  eg:-
  ---------------------
      #!/usr/bin/env node
    // Your code here

2. Interacting with the Command Line:
console.log(): The console.log() function is used to print output to the command line. 
It can handle strings, numbers, objects, and more.

eg:-
---------------------------
    console.log("Hello, command line!");
    const x = 10;
    console.log("The value of x is:", x); 

Command-line arguments: Access command-line arguments passed to the script using process.argv.
    console.log(process.argv);

    The first two elements of the array are the node executable path and the script path, respectively.
    The remaining elements are the arguments.
    -> Using child_process module: Execute shell commands from within Node.js using the child_process module.

    example:-
    --------------------------------------
        const { exec } = require('child_process');
    exec('ls -l', (error, stdout, stderr) => {
      if (error) {
        console.error(`Error: ${error}`);
        return;
      }
      console.log(`stdout: ${stdout}`);
      console.error(`stderr: ${stderr}`);
    });
    --------------------------------------

    3. Node.js REPL (Read-Eval-Print Loop):
The Node.js REPL is an interactive environment for executing JavaScript code.
To start the REPL, type node in your terminal without any file argument.
You can enter JavaScript code directly and it will be executed immediately.
Dot commands: REPL has special dot commands for various actions:
.help: Shows help for dot commands.
.editor: Enters editor mode for multi-line input.
.clear: Clears the REPL context.
.load: Loads a JavaScript file.
.save: Saves the REPL session to a file.
.exit: Exits the REPL.

    4. Packages for Command Line Interfaces:
commander: A popular library to handle command-line arguments.
yargs: Another library for parsing command-line arguments.
These methods enable you to create powerful command-line tools and interact with the system using Node.js.

Q13) Transaction management in node js?
Ans:- Transaction management in Node.js, particularly when interacting with databases, ensures data integrity 
and consistency by treating a series of operations as a single, atomic unit. This means either all operations 
within a transaction succeed and are committed, or if any operation fails, the entire transaction is rolled back,
and no changes are made to the database.

Key principles and steps for managing transactions in Node.js:
Database Driver/ORM Integration:
Most database drivers (e.g., mysql, pg for PostgreSQL, mongodb for MongoDB) and Object-Relational Mappers (ORMs)
like Sequelize or Mongoose provide built-in support for transactions.
These tools expose methods to initiate, commit, and roll back transactions.

Starting a Transaction: 
A transaction is typically initiated by calling a specific method on the database connection or session object 
(e.g., beginTransaction() in SQL-based databases, startSession() and startTransaction() in MongoDB).

Executing Transactional Queries/Operations:
All database operations intended to be part of the atomic unit are executed within the context of the active transaction.
This often involves passing a transaction object or session to the individual query or operation calls.

Committing the Transaction:
If all operations within the transaction execute successfully, the changes are made permanent in the database by committing 
the transaction (e.g., commit() or commitTransaction()).

Rolling Back the Transaction:
If any error occurs during the execution of operations within the transaction, or if a specific condition is not met, the
transaction is aborted, and all changes made during that transaction are undone (e.g., rollback() or abortTransaction()). 
This ensures that the database remains in a consistent state.

Error Handling and Session Management:
Robust error handling is crucial to catch exceptions and trigger rollbacks when necessary.
For session-based transactions (common in NoSQL databases like MongoDB), ending the session after the transaction is 
committed or aborted is important for resource management.

Example (Conceptual with a SQL-like database):
------------------------------------------------------------------------------------
const connection = await pool.getConnection(); // Get a database connection
try {
    await connection.beginTransaction(); // Start the transaction

    // Execute multiple database operations within the transaction
    await connection.query('UPDATE accounts SET balance = balance - ? WHERE id = ?', [amount, senderId]);
    await connection.query('UPDATE accounts SET balance = balance + ? WHERE id = ?', [amount, receiverId]);

    await connection.commit(); // Commit the transaction if all operations succeed
    console.log('Transaction committed successfully.');
} catch (error) {
    await connection.rollback(); // Rollback the transaction if an error occurs
    console.error('Transaction rolled back:', error);
} finally {
    connection.release(); // Release the connection back to the pool
}
----------------------------------------------------------------------------------------

Q14) What is webhooks?
Ans:- Webhooks are a method for applications to communicate in real-time by sending automated messages or 
information to other applications when specific events occur. They are essentially user-defined HTTP callbacks 
that get triggered when certain events happen in a system. 

ðŸŸ¢Key Concepts:
Event-driven:
Webhooks are triggered by specific events within an application, such as a new user registration, a payment being processed, 
or a code change being committed. 

Real-time:
Webhooks provide near real-time updates, as the receiving application is notified immediately when the event occurs. 

HTTP callbacks:
Webhooks utilize HTTP requests (typically POST) to send data to a designated URL (the webhook endpoint). 

One-way communication:
Webhooks are unidirectional, meaning data is sent from the source application to the receiving application, unlike WebSockets
which are bidirectional. 

ðŸŸ¢How Webhooks Work:
-------------------------------------------
1. Registration:
The receiving application registers a webhook endpoint (a URL) with the source application, indicating which events it's interested in. 
2. Event Trigger:
When a registered event occurs in the source application, it sends an HTTP POST request to the registered webhook URL. 
3. Payload Delivery:
The POST request contains a payload of data related to the event. 
4. Processing:
The receiving application processes the data in the payload and takes appropriate action, such as updating a database, sending 
a notification, or triggering a workflow. 

ðŸŸ¢Use Cases:
------------------------------------------
Notifications: Sending notifications to users or other applications about events like new orders, code commits, or 
social media mentions. 

Data synchronization: Automatically syncing data between different applications, such as customer data between a CRM and 
an email marketing platform. 

Workflow automation: Triggering automated workflows based on specific events, such as deploying code to a production server 
when a new commit is pushed. 

Continuous Integration/Continuous Deployment (CI/CD): Triggering CI pipelines when code is pushed to a repository. 
Payment processing: Notifying accounting software when a payment is received. 

ðŸŸ¢Benefits of Webhooks:
------------------------------------------
Real-time updates: Receive data as it happens, eliminating the need for polling. 
Reduced overhead: Webhooks are more efficient than polling, as they only send data when events occur. 
Simplified integration: Webhooks provide a simple and standardized way for applications to communicate. 
Event-driven automation: Enable the creation of powerful, automated workflows. 

Security Considerations:
Authentication: Verifying the authenticity of the webhook request is crucial to prevent malicious actors from sending fake events.
Signature validation: Using HMAC algorithms with a shared secret key to verify the request's signature is a common security practice.
Timestamp validation: Validating the timestamp of the request can help prevent replay attacks. 

Q15) What is buffer in memory? 
Ans:- In Node.js, a Buffer is a global object used to handle and manipulate raw binary data directly in memory. It represents a 
fixed-size chunk of memory allocated outside the V8 JavaScript engine's heap.

ðŸŸ¢Key characteristics and uses of Buffers in Node.js:
----------------------------------------------------
Binary Data Handling:
Buffers are specifically designed for working with binary data, such as images, audio/video files, network packets, or raw data 
from I/O operations.

Fixed Size:
Once a Buffer is created, its size cannot be changed. This fixed-size nature allows for efficient memory management when dealing
with large amounts of data.

Direct Memory Access:
Buffers provide a low-level way to interact with raw bytes, allowing for direct manipulation of memory. This is crucial for 
performance-critical operations.

Efficiency:
Buffers are more efficient than JavaScript strings for handling binary data because they avoid the overhead of encoding and 
decoding data into string formats.

I/O Operations:
They are fundamental for handling I/O operations like reading from or writing to files, or sending/receiving data over network connections.

Streams:
Buffers are commonly used in conjunction with Node.js streams, where data is processed in chunks, and Buffers serve as the temporary
storage for these chunks.

ðŸŸ¢Creating Buffers:
----------------------------------
-> Buffer.alloc(size): Creates a new Buffer of a specified size and initializes it with zeros.
-> Buffer.allocUnsafe(size): Creates a new Buffer of a specified size without initializing it. This can be faster but might 
   expose sensitive data from previously allocated memory.
-> Buffer.from(data): Creates a new Buffer from various data sources like strings, arrays, or other Buffers.

ðŸŸ¢Common Buffer Methods:-

->  buf.toString(): Converts the Buffer's content to a string using a specified encoding (e.g., 'utf8', 'hex', 'base64').
->  buf.write(): Writes data to the Buffer.
->  buf.copy(): Copies data from one Buffer to another.
->  buf.length: Returns the size of the Buffer in bytes.
->  In essence, Buffers are a crucial part of Node.js for efficiently managing and interacting with binary data, particularly in scenarios involving I/O and network communication.

Q16) What is Streams in node js ? 
Ans:- Streams in Node.js are a fundamental concept for handling data flow, especially when dealing with large datasets or 
I/O operations like file manipulation and network communication. They represent a collection of data that may not be available 
all at once and doesn't necessarily need to fit entirely in memory.

ðŸŸ¢Key characteristics of Node.js Streams:
-------------------------------------
Efficiency:
Streams process data in chunks, reducing memory consumption and enabling faster processing of large files or network requests 
compared to loading entire datasets into memory.

Asynchronous Nature:
Streams are built upon the EventEmitter class, allowing them to emit various events (like data, end, error, finish) at different 
stages of data processing. This enables asynchronous handling of data as it becomes available.

Piping:
A powerful feature that allows connecting the output of one stream to the input of another, creating a pipeline for data 
transformation and transfer.

ðŸŸ¢Types of Streams in Node.js:
-----------------------------------
Readable Streams:
From which data can be read (e.g., fs.createReadStream() for reading from a file).
Writable Streams:
To which data can be written (e.g., fs.createWriteStream() for writing to a file).
Duplex Streams:
Streams that are both Readable and Writable (e.g., net.Socket for network communication).
Transform Streams:
A type of Duplex stream that can modify or transform the data as it is written and read (e.g., zlib.createDeflate() 
for data compression). 

Example of using Streams (Piping):
-------------------------------------------------------------
const fs = require('fs');

const readableStream = fs.createReadStream('input.txt');
const writableStream = fs.createWriteStream('output.txt');

readableStream.pipe(writableStream); // Pipes data from input.txt to output.txt

readableStream.on('end', () => {
  console.log('File reading complete.');
});

writableStream.on('finish', () => {
  console.log('File writing complete.');
});

readableStream.on('error', (err) => {
  console.error('Error reading file:', err);
});

writableStream.on('error', (err) => {
  console.error('Error writing file:', err);
});
-------------------------------------------------------------

Q17) What is clusture module in node js? 
Ans:- 
Node.js runs on a single thread by default.
The Cluster module lets you create child processes (workers) that share the same server port.
It helps take advantage of multi-core CPUs to improve performance and scalability.

ðŸ”· Why use Cluster?
Node.js is single-threaded, so it can only use one CPU core per process.
So if there is long running task then due to long running task or long running incomming request it should not 
block other request.
Cluster creates multiple worker processes to handle concurrent requests.
If one worker crashes, others can keep running.

ðŸ”· How it works?
The master process spawns multiple worker processes.
Workers share the same server port.
Master distributes incoming connections among workers (round-robin).

ðŸ”· Basic Example of Cluster
-------------------------------------------------------
import cluster from "cluster";
import http from "http";
import os from "os";

if (cluster.isPrimary) {
  const numCPUs = os.cpus().length;
  console.log(`Master process is running. Spawning ${numCPUs} workers.`);

  // Fork workers
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }

  cluster.on(
    "exit",
    (worker: { process: { pid: any } }, code: any, signal: any) => {
      console.log(`Worker ${worker.process.pid} died. Spawning a new one.`);
      cluster.fork(); // Restart worker
    }
  );
} else {
  // Worker processes run the server
  http
    .createServer((req: http.IncomingMessage, res: http.ServerResponse) => {
      if (req.url === "/") {
        res.writeHead(200, { "Content-Type": "text/plain" });
        res.end("Home Page");
      } else if (req.url === "/slowPage") {
        for (let i = 0; i < 90000000000; i++) {}
        res.writeHead(200, { "Content-Type": "text/plain" });
        res.end("Slow Page");
      }
      res.writeHead(200, undefined);
      res.end(`Hello from worker ${process.pid}\n`);
    })
    .listen(8000);

  console.log(`Worker ${process.pid} started`);
}

----------------------------------------------------

ðŸ”· Key Points:
------------------------
| Feature            | Description                           |
| ------------------ | ------------------------------------- |
| `cluster.isPrimary`| True if the current process is master |
| `cluster.fork()`   | Creates a new worker process          |
| Workers share port | All workers listen on the same port   |
| Fault tolerance    | Master can restart workers on failure |
| Load balancing     | Distributes requests across workers   |


ðŸ”· When to Use
For CPU-intensive apps or to handle high traffic.
When you want to utilize all CPU cores in your server.

Q18) What is Distributed Messeging Queues?
Ans:- Distributed Messaging Queues: Kafka and RabbitMQ

### Key Concepts and Definitions

- **Messaging Queue (MQ):** A system where a **producer** publishes messages to a **queue**, and a **consumer** reads and processes these messages asynchronously.
- **Advantages of MQ:**
  - **Asynchronous processing:** Decouples producer and consumer, reducing latency.
  - **Retry capability:** Handles failures by retrying message processing.
  - **Pace matching:** Balances load when producers send messages faster than consumers can process.
  - **Reliability:** Ensures message durability and fault tolerance.

---

### Messaging Patterns

| Pattern          | Description                                                                              | Key Point                                         |
|------------------|------------------------------------------------------------------------------------------|--------------------------------------------------|
| **Point-to-Point**| Message is consumed by **only one consumer** from the queue.                            | Single message â†’ Single consumer                  |
| **Pub/Sub**       | Message is **broadcast** to multiple queues; multiple consumers can process the message.| Single message can be consumed by multiple consumers |

---

### Kafka: Architecture and Components

- **Producer:** Sends messages to Kafka brokers.
- **Broker:** Kafka server that hosts topics.
- **Topic:** Named stream of messages; logical grouping.
- **Partition:** Subdivision of a topic; actual storage unit.
- **Offset:** Position marker within a partition indicating which messages have been consumed.
- **Consumer:** Reads messages from partitions.
- **Consumer Group:** Multiple consumers read partitions in parallel; each partition read by only one consumer in the group.
- **Cluster:** Group of brokers working together.
- **Zookeeper:** Manages broker coordination, metadata, leader election.

**Kafka Message Structure:**

| Field     | Description                                              | Mandatory          |
|-----------|----------------------------------------------------------|--------------------|
| Key       | Identifier for partition assignment (e.g., car ID)       | Optional           |
| Value     | Actual message content                                   | Mandatory          |
| Topic     | Topic name                                               | Mandatory          |
| Partition | Specific partition number                                | Optional           |

**Partition Assignment Logic:**

- If key is present â†’ message assigned to partition via hash of key.
- If key empty but partition specified â†’ message goes to that partition.
- If neither key nor partition specified â†’ round-robin distribution.

---

### Kafka Message Flow and Fault Tolerance

- **Offset Tracking:** Zookeeper tracks committed offset per consumer group, ensuring consumers know where to resume after failure.
- **Consumer Failure:** Other consumers in the group take over partition reading from last committed offset.
- **Leader-Follower Model:** Partitions have a **leader** broker and one or more **replica followers** for fault tolerance. Writes and reads go through the leader; followers replicate data.
- **Cluster Scaling:** Multiple brokers distribute partitions; data and load spread across machines.
- **Handling Queue Size Limit:** Adding more brokers and partitions distributes load and storage.
- **Retry Mechanism:** Failed message processing retries up to a configured limit; unprocessable messages are sent to a **dead letter queue**.

---

### RabbitMQ: Architecture and Differences from Kafka

- **Producer â†’ Exchange â†’ Queue â†’ Consumer** flow.
- **Exchange:** Routes messages to queues based on routing keys and exchange type.
- **Exchange Types:**
  - **Fanout:** Broadcasts messages to all bound queues.
  - **Direct:** Routes messages to queues with exact routing key match.
  - **Topic:** Supports wildcard routing keys for flexible routing.
- **Push-Based Model:** RabbitMQ **pushes** messages to consumers as soon as they arrive.
- **Retry Handling:** If a consumer fails to process a message, it can **requeue** the message for retry. After max retries, message moves to a dead letter queue.
- **No Offset Concept:** Unlike Kafka, RabbitMQ does not track consumer offsets; it relies on message acknowledgments and requeueing.

---

### Use Cases Highlighted

- **E-commerce Notifications:** Asynchronous notification sending to reduce latency and improve user experience.
- **Pace Matching:** Handling varying message production rates with slower consumer processing.
- **Real-time Location Tracking:** In a cab service, thousands of GPS location updates are sent every few seconds. MQs help buffer and pace the data consumption for dashboard updates.

---

### Important Interview Insights

- Understand **difference between point-to-point and pub/sub** messaging.
- Explain Kafkaâ€™s **partition, offset, consumer group, leader-follower replication, and Zookeeper role**.
- Know RabbitMQâ€™s **exchange types and routing mechanisms**.
- Be able to discuss **failure scenarios**:
  - What happens if queue size limit is reached?
  - What if a consumer goes down?
  - How retries and dead letter queues work.
- Understand **Kafka is pull-based** (consumer polls broker) while **RabbitMQ is push-based** (broker pushes message to consumer).

---

### Summary Table: Kafka vs RabbitMQ

| Feature                | Kafka                                      | RabbitMQ                                  |
|------------------------|--------------------------------------------|------------------------------------------ |
| Model                  | Pull-based (consumer polls for messages)   | Push-based (broker pushes messages)       |
| Message Routing        | Topics â†’ Partitions                        | Exchanges â†’ Queues                        |
| Message Ordering       | Partition level ordering                   | Queue level ordering                      |
| Consumer Tracking      | Offset stored in Zookeeper                 | Ack-based, no offsets                     |
| Retry Handling         | Offset management + Dead Letter Queue      | Requeue + Dead Letter Queue               |
| Scalability            | High, via partitions and broker clusters   | Moderate, via queues and exchanges        |
| Use Case Focus         | High throughput, distributed systems       | Flexible routing, complex routing patterns|

---

Q19) suppose there is 10000 user accessing server in node js how can we handle concurrency?
Ans:-  To handle 10,000 concurrent users in a Node.js server, you need to design for high concurrency, non-blocking operations, scalability, and fault-tolerance.
Below is a clear, practical guide you can follow ðŸ‘‡

----------------------------------------------------------------------------------------------------------

âœ… How Node.js Handles Concurrency

Node.js is single-threaded, but it handles multiple requests using:

Event Loop

Non-blocking I/O

Async operations (Promises, async/await)

As long as you avoid blocking the thread, Node.js can handle thousands of connections.

ðŸ”¥ Techniques to Handle 10,000+ Concurrent Users
1. Avoid Blocking Code

Never block the event loop.

â›” Wrong (blocking):
---------------------------------------------------------
const fs = require("fs");
const data = fs.readFileSync("file.txt"); // Blocks thread
---------------------------------------------------------

âœ… Correct (non-blocking):
---------------------------------------------------------
fs.readFile("file.txt", (err, data) => {});
--------------------------------------------------------

Also avoid:

--CPU heavy calculations
--Large loops
--JSON.stringify of big objects
--Sync DB calls


2. Use Cluster Mode (Multi-core CPU Utilization)

Node runs on 1 CPU core only, but with clustering you can use all cores.

Example using PM2:

ðŸ‘‰ PM2 Auto-clustering:
--------------------------------
pm2 start server.js -i max
--------------------------------

This spawns workers equal to number of CPU cores (ex: 8 workers).
ðŸ‘‰ Native cluster module:
--------------------------------
const cluster = require('cluster');
const os = require('os');

if (cluster.isMaster) {
  const cpus = os.cpus().length;
  for (let i = 0; i < cpus; i++) cluster.fork();
} else {
  require("./app");
}
---------------------------------

3. Use Load Balancer (Nginx / AWS ELB)
A load balancer sits in front and distributes traffic:
---------------------------------------------------
Client â†’ Load Balancer â†’ Node.js Cluster â†’ Database
---------------------------------------------------
Nginx example:
---------------------------------------------------
upstream app_cluster {
    server 127.0.0.1:3001;
    server 127.0.0.1:3002;
    server 127.0.0.1:3003;
}
server {
    location / {
        proxy_pass http://app_cluster;
    }
}
---------------------------------------------------


4. Use Message Queues for Heavy Tasks

If your app sends:
-Emails
-PDFs
-Videos
-Push notifications
-Reports

Move it to a queue:

Common queues:
-BullMQ (Redis)
-RabbitMQ
-Kafka

Example job queue (BullMQ):
---------------------------------------------------
queue.add("sendEmail", { email: "test@mail.com" });
---------------------------------------------------
Workers run in background and keep your server free.

5. Use Connection Pooling for Database

Do NOT open/close DB connections per request.
MySQL example:
--------------------------------------------
const pool = mysql.createPool({
  connectionLimit: 20,
});
--------------------------------------------

MongoDB:
Use single shared client:
--------------------------------------------
const client = new MongoClient(uri);
await client.connect();
--------------------------------------------

6. Use Caching (Redis)
Offload repeated queries using cache:
------------------------------------------------
const user = await redis.get("user:123");

if (!user) {
    const result = await db.users.findOne({ id: 123 });
    redis.set("user:123", JSON.stringify(result));
}
--------------------------------------------------
Cache helps reduce DB load drastically.


7. Horizontal Scaling

If traffic grows even more:
--------------------------
Server 1
Server 2
Server 3
Server 4
-------------------------

Use:
-Kubernetes
-Docker Swarm
-AWS Auto Scaling

8. Use Rate Limiting to Protect Server

Prevent one user from overloading your server:
-----------------------------------------------
import rateLimit from 'express-rate-limit';

app.use(rateLimit({
    windowMs: 60_000,
    max: 100
}));
-----------------------------------------------


9. Performance Monitoring

Use:
-PM2 Monitoring
-Grafana + Prometheus
-New Relic
-Datadog


ðŸ”¥ High-Performance Server Template
------------------------------------------------
import express from "express";
import cluster from "cluster";
import os from "os";

if (cluster.isPrimary) {
    const cpuCount = os.cpus().length;
    for (let i = 0; i < cpuCount; i++) cluster.fork();
} else {
    const app = express();
    app.get("/", (req, res) => {
        res.send("Hello 10k users!");
    });
    app.listen(3000);
}
-------------------------------------------------


âœ… Summary (Easy to Understand)

| Technique             | Why Needed?                        |
| --------------------- | ---------------------------------- |
| Non-blocking I/O      | Node can handle thousands of users |
| Clustering            | Use all CPU cores                  |
| Load Balancer         | Split traffic across servers       |
| Message Queues        | Offload heavy tasks                |
| DB connection pooling | Prevent DB overload                |
| Redis caching         | Reduce DB queries                  |
| Horizontal scaling    | Add more servers                   |
| Rate limiting         | Prevent abuse                      |


Q20) Why background jobs needed in node js environment?
Ans:- A background job in Node.js is a task that runs asynchronously and outside the main request/response cycle, so your Node.js server doesn't get blocked.

These jobs are handled by workers, not by the main API thread.

âœ… Simple Explanation

When a user hits your API, you respond immediately and push heavy work to a background worker.

ðŸ‘‰ Main server = handles requests
ðŸ‘‰ Background worker = handles slow tasks

ðŸŽ¯ Why Background Jobs Are Used?
To avoid blocking your Node.js server.

Common use cases:
--------------------
-Sending emails
-Generating PDFs
-Processing images & videos
-Generating reports
-Payment processing
-Sending notifications
-Data cleanup
-Cron jobs (daily tasks)
-These tasks may take 1â€“30 seconds or more, so we send them to the background.

ðŸ”¥ Example Without Background Job (BAD)
----------------------------------------
app.post("/register", async (req, res) => {
  await sendEmail();   // â›” Blocks server
  res.send("User registered");
});
----------------------------------------
If 100 users register, your API becomes slow or crashes.

âš¡ With Background Job (GOOD)
----------------------------------------
app.post("/register", async (req, res) => {
  emailQueue.add({ email: req.body.email });
  res.send("User registered");
});
----------------------------------------

Worker:
----------------------------------------
const worker = new Worker("emailQueue", async job => {
  await sendEmail(job.data.email);
});
----------------------------------------

The server is free instantly.
The worker sends the email in background.

ðŸ’¡ How Background Jobs Work Internally

a) API adds a job to a queue â†’ (Redis / RabbitMQ / Kafka)
b) A separate worker process listens to the queue
c) Worker picks a job
d) Worker executes the heavy task
e) Marks the job completed

ðŸ§° Popular Background Job Libraries in Node.js

1. BullMQ (Redis - most popular)
-Fast
-Easy
-Supports scheduling

Works well with queues & workers

2. Agenda
-Cron + jobs
-Uses MongoDB

3. Bree
Pure Node.js worker threads

4. RabbitMQ
Enterprise-grade message queue


Q20) What is redis ?
Ans:-

- **Problem with Traditional Database Queries:**
  - When a user visits a web page, the server queries multiple tables in a database (e.g., PostgreSQL, MongoDB).
  - These queries often involve numerous foreign key lookups and complex joins, leading to **high response times**.
  - If the user refreshes the page repeatedly, the server reruns the expensive queries multiple times, increasing load and slowing down the user experience.
  - This results in **increased server load, higher latency, and a poor user experience**.

- **Redis as a Solution:**
  - Redis operates as an **in-memory data store** (RAM-based), making data access extremely fast but temporary.
  - It acts as a caching layer between the application server and the database.
  - When data is first queried, Redis stores (caches) the computed result.
  - Subsequent requests for the same data are served from Redis, drastically reducing response time and database load.
  - Redis supports **setting expiry times** on cached data, ensuring stale data is invalidated and fresh data fetched from the database.

---

### Redis Architecture Overview

| Component           | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| User                | Initiates requests by interacting with the application/web server           |
| Application Server  | Processes user requests, queries data from Redis or falls back to the database |
| Database            | Traditional data store (e.g., PostgreSQL, MongoDB) holding persistent data   |
| Redis Server        | In-memory cache server storing frequently accessed or computed data          |

- On a user request, the server first checks Redis.
- If Redis has the cached data, it returns it instantly.
- If not, the server queries the database, caches the result in Redis, and returns it to the user.
- This architecture reduces **database reads** and **improves user response times**.

---

### Redis Features and Data Structures

- **Core Data Types:**
  - **Strings:** Simple key-value pairs, e.g., user names or message counts.
  - **Lists:** Ordered collections supporting push/pop operations on left or right ends, useful for queues or stacks.
  - **Sets:** Unordered collections of unique elements, useful for tracking unique items like IP addresses.
  - **Hashes (HAS):** Key-value pairs within a key, resembling objects or dictionaries, helpful to store related fields like a bike's model, brand, price.
  - **Sorted Sets:** Similar to sets but ordered by a score, useful for priority queues or rankings.
  - **Streams:** Append-only log data structures designed for high-throughput data ingestion, like sensor data or user event tracking.
  - **Geospatial Indexes:** Support storing and querying geo-coordinates, enabling location-based queries such as finding nearby restaurants or hotels.

- **Operations and Commands:**
  - **SET, GET:** Store and retrieve string values.
  - **EXPIRE:** Set expiry time on keys to automatically invalidate cache.
  - **INCR/DECR:** Increment or decrement numeric values atomically.
  - **LPUSH, RPUSH, LPOP, RPOP:** Add or remove items from lists.
  - **SADD, SREM, SISMEMBER:** Manage set membership.
  - **ZRANGE, ZADD:** Manage sorted sets with scores.
  - **XADD, XRANGE:** Add and read entries from streams.
  - **GEOADD, GEORADIUS:** Manage geospatial data and perform radius searches.

---

### Practical Implementation and Best Practices

- **Installation and Setup:**
  - Redis can be installed on various platforms (Linux, MacOS, Windows).
  - Recommended to run Redis inside a Docker container for portability and consistency across environments.
  - Redis server runs by default on port **6379**, and a GUI client like **RedisInsight** on port **8001** helps visualize and interact with data.

- **Client Integration:**
  - Examples provided use Node.js with the `ioredis` package to interact with Redis.
  - Basic operations demonstrated:
    - Connecting to Redis server.
    - Setting keys and values.
    - Getting values.
    - Using expiry to invalidate cache.
    - Using list commands to implement queues.
    - Using sets and hashes for structured data.
  - Proper **key-naming conventions** (e.g., `user:1:name`) help organize and group keys for clarity and efficient querying.

- **Caching Example:**
  - A sample Express.js server is built to demonstrate caching API responses:
    - On request, check Redis cache.
    - If cache miss, fetch from database (mocked), cache the result with expiry, and return.
    - Subsequent requests serve data from Redis cache instantly.
  - Demonstrates **dramatic reduction in response time** (from seconds to milliseconds).

---

### Advanced Usage and Features

- **Streams:** Efficiently handle high-throughput data like logs, sensor readings, or user actions, supporting append-only data with timestamped entries.
- **Geospatial Queries:** Store and query location-based data efficiently, useful for applications like delivery services or hotel finders.
- **Pub/Sub Model:** Redis supports publish-subscribe messaging to build scalable real-time features like notifications.
- **Rate Limiting:** Can be implemented using Redis counters with expiry to control API usage or user actions.

---

### Summary of Benefits of Redis

- **High performance due to in-memory storage.**
- **Significant reduction in database load by caching computed or frequently accessed data.**
- **Improved user experience via faster response times.**
- **Rich data types and atomic operations enable flexible application logic.**
- **Supports expiration and invalidation to keep cache fresh.**
- **Easy integration with modern tech stacks and cross-platform support.**
- **Tools like RedisInsight facilitate monitoring and debugging.**

---

### References for Further Learning

- Official Redis documentation and commands reference.
- Node.js `ioredis` client documentation.
- Docker-based Redis deployment.
- RedisInsight GUI for data visualization.

---

Q21) What is proxy server?
Ans:-
 **proxy server** is an **intermediary hardware or software component** placed between a client and an application or server to provide intermediate services during communication.

### Key Concepts

- **Proxy Server**: A device or software that acts as a middleman between a client and a server.
- **Positioning**: It is placed between the client and the server to manage or modify requests and responses.
- **Two Types of Proxies**:
  - **Forward Proxy**
  - **Reverse Proxy**

### Detailed Explanation of Forward Proxy
---------------------------------------------------------------------------------------------------------------------

- When a client (e.g., a mobile application) sends a request to a server, normally the connection is direct. Both client and server know each other's identity.
- By introducing a **forward proxy**, the client sends requests to the proxy server instead of directly to the actual server.
- The proxy server then forwards the request to the real server.
- The **real server does not know the identity of the original client**; it only sees the proxy serverâ€™s request.
- The proxy server **hides the client's identity** from the actual server, acting as a shield.

### Practical Use Case

- Understand a situation where certain websites are **blocked in India**.
- A user attempts to access such a blocked website directly and encounters restrictions.
- By using a **forward proxy**, the user sends requests through the proxy server instead of directly to the blocked website.
- The blocked website only sees the proxy server's location (which could be outside India) and allows the request.
- This allows the user to access the blocked content **without revealing their real location or identity** to the blocked website.

### Important Points

- **Forward proxy** hides the clientâ€™s identity from the server.
- The server believes the request is coming from the proxy server, not from the client.
- This method is often used to bypass **geographical restrictions or content blocks**.
- The actual server responds to the proxy server, which then forwards the response back to the client.
- The video promises to explain the **reverse proxy** in the next session.

### Glossary Table

| Term           | Definition                                                     |
|----------------|----------------------------------------------------------------|
| Proxy Server   | An intermediary device/software that relays requests between client and server. |
| Forward Proxy  | A proxy server that hides the client's identity from the actual server.          |
| Client         | The user or device initiating a request to a server.                           |
| Server         | The destination system that processes client requests and sends responses.    |
| IP Blocking    | A method used by servers to restrict access to users from certain geographic locations. |

### Key Insights

- **Proxy servers act as intermediaries, enhancing privacy and security by hiding client identities.**
- **Forward proxies are commonly used to bypass content restrictions imposed by geographic or network policies.**
- **The actual server only sees the proxy serverâ€™s IP and not the clientâ€™s, enabling anonymity.**
- **This mechanism is useful in scenarios involving censorship or content filtering.**



**Detailed Explaination of Reverse Proxy
--------------------------------------------------------------------------

### Key Concepts and Definitions

| Term            | Definition                                                                                       |
|-----------------|-------------------------------------------------------------------------------------------------|
| **Forward Proxy** | A server that hides the identity of the client from the actual server. For example, accessing a blocked website in India via a proxy server       located elsewhere, so the target server sees the request coming from the proxy, not the client. |
| **Reverse Proxy** | A server that hides the identity of the actual server from the client. It acts as an intermediary, presenting a single interface while routing requests internally to multiple backend servers. |

---

### Core Explanation

- **Forward Proxy**: 
  - Hides the clientâ€™s identity.
  - Example: If a website is blocked in India, a forward proxy server in another country fetches the website content, so the website does not know the client is in India.
  
- **Reverse Proxy**:
  - Hides the serverâ€™s identity.
  - Acts as an abstraction layer between clients and multiple backend servers.
  - Clients see only the reverse proxyâ€™s IP address, not the actual backend server IPs.
  - Useful for scalability and security by preventing direct access to internal servers.
  
---

### Practical Example: Amazon.com

- Amazon has numerous backend servers handling different functions like orders, payments, and cart management.
- Instead of exposing multiple IPs and servers, **Amazon uses a reverse proxy** to present a single domain (amazon.com).
- Internally, the reverse proxy routes requests to the appropriate backend server:
  - Payment requests â†’ payment server
  - Order requests â†’ order server
  - Cart requests â†’ cart server
- This setup **simplifies complexity and hides internal architecture** from clients.
- The reverse proxy also enables:
  - **Load balancing**: Distributing traffic across multiple servers.
  - **Server swapping**: Admins can change backend servers without disrupting service.
  - **Security controls and filtering**: For example, only allowing certain HTTP methods (e.g., GET but blocking POST).

---

### Additional Details

- Reverse proxies can filter or block unwanted requests before they reach backend servers.
- Example: If a user tries to change subscription plans with an unauthorized POST request, the reverse proxy can block it.
- The concept is similar to an **API Gateway**, which sits between clients and servers, routing requests based on predefined mappings.
- API Gateway maintains a mapping of virtual IPs and endpoints to actual backend services.
  
---

### Summary of Benefits of Reverse Proxy

- **Hides backend server IPs** from the client.
- **Simplifies client interaction** by exposing a single domain or IP.
- Provides **load balancing and fault tolerance**.
- Enables **request filtering and security enforcement**.
- Allows **seamless backend server management** without client disruption.

---

### Key Insights

- Reverse proxy is an essential component in modern web architectures, especially for large platforms like Amazon.
- It provides an abstraction layer that enhances security, scalability, and manageability.
- It differs fundamentally from forward proxy by hiding the server rather than the client.
- Reverse proxies often integrate with API gateways to manage and route API requests efficiently.

---

Q22) Provide Security Machansm in node js?
Ans:- There are lots of way from which you can implement security machanism.

1. Authentication & Authorization ðŸ”
Authentication (Who are you?)

-JWT (JSON Web Token) â€“ most common
-OAuth2 (Google, GitHub login)
-Session-based auth (cookies + Redis)

Best practices
---------------------------------------------
// JWT verification middleware
const jwt = require("jsonwebtoken");

function auth(req, res, next) {
  const token = req.headers.authorization?.split(" ")[1];
  if (!token) return res.status(401).send("Unauthorized");

  jwt.verify(token, process.env.JWT_SECRET, (err, user) => {
    if (err) return res.status(403).send("Invalid token");
    req.user = user;
    next();
  });
}
---------------------------------------------

Authorization (What can you do?)

-RBAC (Role Based Access Control)
-ABAC (Attribute based)
------------------------------------------------
const isAdmin = (req, res, next) => {
  if (req.user.role !== "admin") return res.status(403).send("Forbidden");
  next();
};
------------------------------------------------

2. Input Validation & Sanitization ðŸ§¹

Prevents SQL Injection, NoSQL Injection, XSS

Tools
-Joi, Zod, express-validator
-MongoDB: prevent $ne, $gt attacks

3. Prevent XSS (Cross-Site Scripting) ðŸ›¡ï¸

Backend
-Escape user input
-Never trust frontend data

Frontend (React)
-React auto-escapes JSX (safe by default)
-Avoid dangerouslySetInnerHTML

Headers
---------------------------
const helmet = require("helmet");
app.use(helmet());
---------------------------

4. Prevent CSRF (Cross-Site Request Forgery) ðŸ”„

Important when using cookies

Solutions
-CSRF token
-SameSite cookies
------------------------------------------
app.use(csrf({ cookie: true }));

res.cookie("token", jwt, {
  httpOnly: true,
  sameSite: "strict",
  secure: true
});
------------------------------------------

5. Secure Password Handling ðŸ”‘

âŒ Never store plain passwords
âœ… Always hash passwords
------------------------------------------
const bcrypt = require("bcrypt");

const hash = await bcrypt.hash(password, 12);
const isValid = await bcrypt.compare(password, hash);
------------------------------------------

6. Rate Limiting & Brute-Force Protection ðŸš¦

Prevents DDoS & password attacks
------------------------------------------
const rateLimit = require("express-rate-limit");

app.use("/login",
  rateLimit({
    windowMs: 15 * 60 * 1000,
    max: 5
  })
);
------------------------------------------

7. Secure HTTP Headers (Helmet) ðŸª–

Protects from:
-Clickjacking
-MIME sniffing
-XSS
-------------------
app.use(helmet());
-------------------

8. CORS Security ðŸŒ

Allow only trusted domains
----------------------------
const cors = require("cors");

app.use(cors({
  origin: ["https://myfrontend.com"],
  credentials: true
}));
----------------------------

9. Protect Against NoSQL Injection (MongoDB) ðŸƒ
----------------------------------------
// BAD
User.findOne({ email: req.body.email });

// GOOD
User.findOne({ email: String(req.body.email) });
-----------------------------------------

Use
------------------------------------------------------
const mongoSanitize = require("express-mongo-sanitize");
app.use(mongoSanitize());
------------------------------------------------------

10. Environment Variable Security ðŸ”

-Never push secrets to GitHub
-Use .env
----------------------------------
require("dotenv").config();
-----------------------------------

11. HTTPS & TLS ðŸ”’

-Always use HTTPS in production
-Use Nginx / Load Balancer
-------------------------------
app.enable("trust proxy");
-------------------------------

12. Logging & Monitoring ðŸ‘€

Detect attacks early
-Winston / Pino
-Datadog / Prometheus

13. Secure File Uploads ðŸ“

-Validate file type
-Limit file size
-Store outside root
-------------------------------------------------------------
if (!["image/png", "image/jpeg"].includes(file.mimetype)) {
  throw new Error("Invalid file type");
}
-------------------------------------------------------------

Q23) How you can upload files to AWS S3 using Node js?
Ans:-
### Core Workflow Overview

- **Client-side (React app)**: Users upload image files along with captions.
- **Express server (Node.js)**: 
  - Receives multi-part form data uploads.
  - Uses middleware (`multer`) configured for memory storage to handle image files in memory.
  - Optionally modifies images (e.g., resizing using `sharp`).
  - Uploads images to an S3 bucket with unique filenames.
  - Stores post metadata (caption, image filename) in a database via Prisma ORM.
  - Generates **signed URLs** for secure, temporary access to private S3 images.
  - Handles deletion requests by removing images from S3 and deleting database entries.

---

### Detailed Implementation Steps

#### 1. Handling File Uploads with Express and Multer

- **Multi-part form data** is necessary for uploading files from the client.
- Express does not parse multi-part data by default, so the **`multer`** middleware is installed and configured.
- Multer is set to **memory storage**, so files are kept in memory buffers for easy manipulation before upload.
- Uploaded image data is accessed via `req.file.buffer` and metadata via `req.body`.

#### 2. Setting Up and Configuring an Amazon S3 Bucket

- The S3 bucket acts as a **cloud-based hard drive**, storing files cheaply and scalably.
- Bucket is created via AWS Console, configured as **private** by default.
- Bucket name and region are saved in environment variables for server access.
- IAM (Identity and Access Management) is used to:
  - Create a **policy** granting minimal permissions (`GetObject`, `PutObject`, `DeleteObject`) restricted to the specific bucket.
  - Create a **user** representing the web app with **programmatic access keys**.
  - Attach the policy to the user and store access keys in environment variables.

#### 3. Uploading Images to S3

- Using the **AWS SDK for JavaScript (v3)**, specifically the **S3Client** and **PutObjectCommand**, the server uploads images.
- Images are assigned **unique filenames** generated via Node.jsâ€™s `crypto` module to avoid overwriting existing files.
- Image metadata (`Content-Type`) is sent along with the file buffer.
- Upon successful upload, the post's data (caption, image filename) is saved in the database.

#### 4. Modifying Images Before Upload

- The library **`sharp`** is used to resize images (e.g., to 1920x1080 portrait format).
- The resized image buffer replaces the original buffer before uploading to S3.

#### 5. Retrieving Images Securely via Signed URLs

- S3 buckets are private, so direct public access to images is not possible.
- The server uses the **S3 Request Presigner** (`getSignedUrl` function) to generate **time-limited URLs**.
- These signed URLs allow clients to access images securely for a limited period (default 1 hour).
- URLs are generated dynamically when fetching posts to ensure access control (e.g., user permissions can be enforced).

#### 6. Deleting Images and Posts

- Client sends a delete request for a post.
- Server finds the post in the database and deletes the corresponding image file from S3 using `DeleteObjectCommand`.
- Once the image is deleted from S3, the database post is deleted.
- This ensures consistency between storage and database.

---

### Important Technical Details & Best Practices

| Aspect                      | Details/Best Practices                                             |
|-----------------------------|------------------------------------------------------------------  |
| File Upload Form Encoding   | Must use `multipart/form-data`                                     |
| Image Filename Strategy     | Use random hex strings (via `crypto`) to avoid collisions          |
| S3 Bucket Permissions       | Grant minimal necessary permissions with IAM policies              |
| Middleware                  | Use `multer` with **memory storage** for in-memory file handling   |
| Image Processing            | Use `sharp` for resizing/modification                              |
| Access Control              | Use signed URLs for secure, temporary access to private S3 objects |
| Environment Variables       | Store bucket name, region, IAM keys securely                       |
| Database                    | Store metadata (image name, caption) separately from image files   |

---

### Summary Timeline of Key Operations

| Step                         | Description                                                                  |
|------------------------------|----------------------------------------------------------------              |
| Upload Image                 | Client sends image + caption as multi-part form data to server               |
| Parse & Modify Image         | Server uses `multer` (memory storage), optionally resizes image with `sharp` |
| Generate Unique Filename     | Server creates unguessable filename with `crypto`                            |
| Upload to S3                 | Server uploads image buffer to private S3 bucket using AWS SDK               |
| Save Post Metadata           | Server saves caption + filename in database via Prisma                       |
| Retrieve Posts               | Server fetches posts, generates signed URLs for images                       |
| Client Displays Images       | Client uses signed URLs to display images securely                           |
| Delete Post & Image          | Server deletes both image from S3 and metadata from database                 |

---

### Key Insights and Conclusions

- **Using S3 for file storage decouples static assets from the application server**, enabling scalability.
- **`multer` middleware with memory storage** allows efficient in-memory image processing before upload.
- **IAM policies should be tightly scoped** to minimize security risks.
- **Unique filenames prevent accidental overwrites in S3** and improve security.
- **Signed URLs provide secure, temporary access** to private images, enabling fine-grained access control.
- The workflow supports all CRUD operations for posts with images: create, read (with secure access), and delete.
- Integration with AWS CloudFront CDN (discussed as a next step) can improve global delivery performance by caching images closer to users.

Q24) How you can setup CDN like Amazon CLOUDFRONT?
Ans:-

### Key Concepts and Process Overview

- **Purpose of CloudFront CDN**:  
  - Distributes S3 bucket content globally to edge locations.  
  - Reduces latency by serving files closer to users geographically.  
  - Enables faster â€œfirst byteâ€ retrieval times compared to direct S3 access.

- **Initial Performance Check**:  
  - Direct S3 bucket requests show varying latency depending on user location.  
  - Users farther from the bucketâ€™s region (e.g., Singapore) experience delays up to one second for the first byte.

- **CloudFront Distribution Setup**:  
  1. Select the origin domain as the existing S3 bucket.  
  2. Enable **Origin Access Identity (OAI)** to keep the bucket private and restrict direct access, allowing only CloudFront to fetch files.  
  3. Leave most settings as defaults:  
      - Default cache policy (24 hours caching).  
      - Redirect HTTP to HTTPS.  
      - Allowed HTTP methods: GET and HEAD only.  
      - No custom headers or origin shield (to reduce cost).  
      - Distribution enabled for all edge locations globally.  
  4. Avoid adding custom domains or IP restrictions for this basic setup.  
  5. Allow automatic update of bucket policy to facilitate OAI permissions.

- **Deployment**:  
  - CloudFront distribution creation takes several minutes as it propagates globally.

---

### Performance Improvement After CloudFront Setup

- Initial request to CloudFront distribution may be similar in speed to S3, but subsequent requests are **significantly faster** due to cached content at edge locations.  
- Measured latency drops to milliseconds for first byte delivery from CloudFront, drastically improving user experience worldwide.

---

### Integration into Node.js Backend

- Previously, image URLs were generated using **signed URLs from S3**.  
- Now, the backend appends the **CloudFront distribution domain** to the image object name (key) from the bucket, forming a public URL that serves cached content.  
- Image uploads still go directly to the private S3 bucket unchanged.  
- Image retrievals happen via CloudFront URLs, improving read efficiency, which is critical for apps with heavy image views like Instagram.

---

### Important Considerations and Limitations

- **Security and URL Signing**:  
  - CloudFront URLs are **not signed** by default in this setup, meaning the URLs are permanent and accessible by anyone who has them.  
    This can pose security risks or unwanted access. 
  

- **Cache Invalidation**:  
  - When an image is deleted from S3, it **remains available in CloudFrontâ€™s cache for 24 hours** due to the default caching policy.  
  - This means deleted content can still be accessed via CloudFront URLs until cache expires.  
  - To immediately update or remove cached files, **manual cache invalidation** is required.


### Summary of AWS Services and Features Used

| Feature/Service                    | Purpose/Description                                                  |
|------------------------------------|-----------------------------------------------------------------     |
| Amazon S3 Bucket                   | Stores private image files, accessed via signed URLs                 |
| AWS CloudFront Distribution        | CDN to cache and serve files globally, reducing latency              |
| Origin Access Identity (OAI)       | Restricts direct S3 access, only allows CloudFront to access         |
| Signed URLs (S3)                   | Secure, time-limited URLs to access private S3 objects               |
| Default Cache Policy               | Caches files at edge locations for 24 hours                          |
| Cache Invalidation (*Not covered*) | Required to purge cached content immediately after updates/deletions |

---

### Key Insights

- **CloudFront significantly improves global file delivery speed** by caching content at edge locations, drastically reducing latency compared to direct S3 access.  
- Using **Origin Access Identity (OAI)** is essential for keeping S3 buckets private while still allowing CloudFront to serve files securely.  
- Integration into backend code involves replacing signed S3 URLs with CloudFront URLs by appending object keys to the CDN domain.  
- **Reads (file access) benefit more from CDN caching than writes (uploads),** which remain direct to S3.  
- Default CloudFront caching can cause deleted or updated files to remain accessible until cache expires, necessitating cache invalidation for immediate consistency.  
- Security concerns with unsigned CloudFront URLs require further setup to implement signed URLs for restricted access, planned for future coverage.

---